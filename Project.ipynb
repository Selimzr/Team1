{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Comments: Identifying Insulting Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Project description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the digital age, with the rise of online platforms and social media, it's crucial to ensure that user-generated content maintains a certain level of decorum. With this project, we aim to analyze a dataset containing comments, indicating whether a comment is insulting or not. Through data analysis and modeling, we aspire to predict and classify comments into these categories, thus automating the process of content moderation to a certain degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Project description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection: Gather a dataset containing user comments,\n",
    "Data Preprocessing: Prepare the dataset for analysis by performing text preprocessing, which includes tasks such as tokenization, lowercasing, removal of punctuation, and stop word removal.\n",
    "\n",
    "Exploratory Data Analysis: Conduct exploratory data analysis to gain insights into the distribution of comments, temporal patterns, and other relevant trends in the dataset.\n",
    "\n",
    "Feature Engineering: Create appropriate features from the text data, such as word embeddings, n-grams, and other relevant linguistic features.\n",
    "\n",
    "Machine Learning Models: Develop and train machine learning models to predict and classify comments into insulting and non-insulting categories. Evaluate the model's performance through metrics such as accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames with the Needed Input Files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Load the dataset from a CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'id': 'id', 'Comment': 'TextComment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CommentLength'] = df['TextComment'].apply(len)\n",
    "df_len = len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Columns description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'id': A unique identifier for each comment.\n",
    "- 'TextComment': The raw text content of the user's comment.\n",
    "- 'CommentLength': The length (number of characters) of each comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preparation and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the text by removing punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", 'TextComment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to remove certain characters from a sentence before tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters_before_tokenization(sentence,keep_apostrophes=False):\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]' \n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    else:\n",
    "        PATTERN = r'[^a-zA-Z0-9 ]' \n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text1 = [remove_characters_before_tokenization(i) for i in clean_text ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-87-eebb3012af83>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-87-eebb3012af83>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    tokens = word_tokenize(text)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#def nlp_preprocessing(text):\n",
    "    \n",
    "#Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "#Convert Tokens to lower case\n",
    "\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "#Remove stopwords common words that do not carry significant meaning  \n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "#Lemmatize the tokens   \n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "#Remove non-alphabetic tokens\n",
    "\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    \n",
    "#Join the processed tokens back into a single string\n",
    "    \n",
    "    all_comments= ' '.join(tokens)\n",
    "    \n",
    "    return all_comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the NLP preprocessing function to each comment and create a new column 'ProcessedComment'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_val_df=df.isnull().sum(axis=0)/df_len\n",
    "miss_val_df\n",
    "\n",
    "df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>TextComment</th>\n",
       "      <th>ProcessedComment</th>\n",
       "      <th>CommentLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"like this if you are a tribe fan\"</td>\n",
       "      <td>like tribe fan</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>\"you're idiot.......................\"</td>\n",
       "      <td>idiot</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>\"I am a woman Babs, and the only \"war on women...</td>\n",
       "      <td>woman babs war woman see coming jackazzes like...</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\"WOW &amp; YOU BENEFITTED SO MANY WINS THIS YEAR F...</td>\n",
       "      <td>wow benefitted many win year bat nice stupid</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>\"haha green me red you now loser whos winning ...</td>\n",
       "      <td>haha green red loser who winning moron</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>\"\\nMe and God both hate-faggots.\\n\\nWhat's the...</td>\n",
       "      <td>god difference refrigerator fart put meat</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>\"Oh go kiss the ass of a goat....and you DUMMY...</td>\n",
       "      <td>oh go kiss as goat dummycraps insult veteran e...</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>\"Not a chance Kid, you're wrong.\"</td>\n",
       "      <td>chance kid wrong</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>\"On Some real Shit FUck LIVE JASMIN!!!\"</td>\n",
       "      <td>real shit fuck live jasmin</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>\"ok but where the hell was it released?you all...</td>\n",
       "      <td>ok hell released copy article pther anyone fuc...</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>\"\\xa0@CrankyVince has found our slice of the I...</td>\n",
       "      <td>crankyvince found slice internet fuck</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>\"\\xa0HAHAHAHAH, you are a delusional moron.\"</td>\n",
       "      <td>delusional moron</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>\"There you are, been missing you.  How are you...</td>\n",
       "      <td>missing feeling better</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>\"wenttoau\\n\\nHe made a prediction just like yo...</td>\n",
       "      <td>made prediction like know anything football ma...</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>\"KATHY LEE....SHUT YOUR PIE HOLE...AND WHATS W...</td>\n",
       "      <td>kathy lee shut pie hole whats face fish lip du...</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>\"You're a crank.\"</td>\n",
       "      <td>crank</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>\"Come on oppa you're a man. XD\"</td>\n",
       "      <td>come oppa man xd</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>\"I like your enthusiasm and initiative. You're...</td>\n",
       "      <td>like enthusiasm initiative real go getter</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>\"Juggalo you suck at trolling \"</td>\n",
       "      <td>juggalo suck trolling</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>\"you seem a very eloquent, grounded and educat...</td>\n",
       "      <td>seem eloquent grounded educated</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>\"Its a wild wild world, keep your dirty finger...</td>\n",
       "      <td>wild wild world keep dirty finger human right</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>\"Are you about to recapture your swag?\"</td>\n",
       "      <td>recapture swag</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>\"no shit.. you must be a\\xa0genius\\xa0\"</td>\n",
       "      <td>shit must</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>\"N\\xe3o tenho a m\\xednima paci\\xeancia pra ess...</td>\n",
       "      <td>tenho pra esse tipo de babaca que acha que se ...</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>\"are you on 'roids by chance?\"</td>\n",
       "      <td>chance</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>\"\\xa0You are a disgusting human being\"</td>\n",
       "      <td>disgusting human</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>\"you are well versed when it may be on. \\xa0Mu...</td>\n",
       "      <td>well versed may one favs know detail</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>\"we?  you and the mouse in your pocket?\"</td>\n",
       "      <td>mouse pocket</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>\"glad you realize this :P\\n\\nkidding. \"</td>\n",
       "      <td>glad realize</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>\"Damn! You would rock this so hard!\"</td>\n",
       "      <td>damn would rock hard</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>\"In your face Cano haters \"</td>\n",
       "      <td>face cano hater</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>\"WHAT FOUL ARE YOU KIDDING!!!!!!!!\"</td>\n",
       "      <td>foul kidding</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>\"You would know alot about rolls, fat boy\"</td>\n",
       "      <td>would know alot roll fat boy</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>\"You look pretty damn good! ^_^\"</td>\n",
       "      <td>look pretty damn good</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>\"You are a moron. I am a computer guy that has...</td>\n",
       "      <td>moron computer guy company mouth get trouble d...</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>\"It is the corrupt government blasting undergr...</td>\n",
       "      <td>corrupt government blasting underground bunker...</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>\"the phillies are terrible. go back to your cave\"</td>\n",
       "      <td>phillies terrible go back cave</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>\"You are so naive.....useful idiot.\"</td>\n",
       "      <td>naive useful idiot</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>\"Are you kidding me?\"</td>\n",
       "      <td>kidding</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>\"You are an opinionated, uneducated idiot. And...</td>\n",
       "      <td>opinionated uneducated idiot kingdom heaven di...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>\"Shut up muffin you soft a** b*tch\"</td>\n",
       "      <td>shut muffin soft b tch</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>\"Okay, I'm going to blow your fucking minds. \\...</td>\n",
       "      <td>okay going blow fucking mind three elderly mem...</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>\"It's also a synonym for shit. \"</td>\n",
       "      <td>also synonym</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>\"Well then you're welcome to contact your own ...</td>\n",
       "      <td>well welcome contact lawyer</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>\"\\xa0@Presidentjlh\\xa0\\xa0@JenBroflovski you c...</td>\n",
       "      <td>jenbroflovski ask mom</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>\"Jimmy, your pop ups and fly outs are the reas...</td>\n",
       "      <td>jimmy pop ups fly out reason never happy</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>\"Well i got on here, but i think im might just...</td>\n",
       "      <td>well got think im might get back see delusion ...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>\"You are so strong\"</td>\n",
       "      <td>strong</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>\"Coach, please stop trying to explain this awa...</td>\n",
       "      <td>coach please stop trying explain away deeper h...</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>\"Learn reading comprehension. Sure Urlacher is...</td>\n",
       "      <td>learn reading comprehension sure urlacher aver...</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                        TextComment  \\\n",
       "0    1                 \"like this if you are a tribe fan\"   \n",
       "1    2              \"you're idiot.......................\"   \n",
       "2    3  \"I am a woman Babs, and the only \"war on women...   \n",
       "3    4  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...   \n",
       "4    5  \"haha green me red you now loser whos winning ...   \n",
       "5    6  \"\\nMe and God both hate-faggots.\\n\\nWhat's the...   \n",
       "6    7  \"Oh go kiss the ass of a goat....and you DUMMY...   \n",
       "7    8                  \"Not a chance Kid, you're wrong.\"   \n",
       "8    9            \"On Some real Shit FUck LIVE JASMIN!!!\"   \n",
       "9   10  \"ok but where the hell was it released?you all...   \n",
       "10  11  \"\\xa0@CrankyVince has found our slice of the I...   \n",
       "11  12       \"\\xa0HAHAHAHAH, you are a delusional moron.\"   \n",
       "12  13  \"There you are, been missing you.  How are you...   \n",
       "13  14  \"wenttoau\\n\\nHe made a prediction just like yo...   \n",
       "14  15  \"KATHY LEE....SHUT YOUR PIE HOLE...AND WHATS W...   \n",
       "15  16                                  \"You're a crank.\"   \n",
       "16  17                    \"Come on oppa you're a man. XD\"   \n",
       "17  18  \"I like your enthusiasm and initiative. You're...   \n",
       "18  19                    \"Juggalo you suck at trolling \"   \n",
       "19  20  \"you seem a very eloquent, grounded and educat...   \n",
       "20  21  \"Its a wild wild world, keep your dirty finger...   \n",
       "21  22            \"Are you about to recapture your swag?\"   \n",
       "22  23            \"no shit.. you must be a\\xa0genius\\xa0\"   \n",
       "23  24  \"N\\xe3o tenho a m\\xednima paci\\xeancia pra ess...   \n",
       "24  25                     \"are you on 'roids by chance?\"   \n",
       "25  26             \"\\xa0You are a disgusting human being\"   \n",
       "26  27  \"you are well versed when it may be on. \\xa0Mu...   \n",
       "27  28           \"we?  you and the mouse in your pocket?\"   \n",
       "28  29            \"glad you realize this :P\\n\\nkidding. \"   \n",
       "29  30               \"Damn! You would rock this so hard!\"   \n",
       "30  31                        \"In your face Cano haters \"   \n",
       "31  32                \"WHAT FOUL ARE YOU KIDDING!!!!!!!!\"   \n",
       "32  33         \"You would know alot about rolls, fat boy\"   \n",
       "33  34                   \"You look pretty damn good! ^_^\"   \n",
       "34  35  \"You are a moron. I am a computer guy that has...   \n",
       "35  36  \"It is the corrupt government blasting undergr...   \n",
       "36  37  \"the phillies are terrible. go back to your cave\"   \n",
       "37  38               \"You are so naive.....useful idiot.\"   \n",
       "38  39                              \"Are you kidding me?\"   \n",
       "39  40  \"You are an opinionated, uneducated idiot. And...   \n",
       "40  41                \"Shut up muffin you soft a** b*tch\"   \n",
       "41  42  \"Okay, I'm going to blow your fucking minds. \\...   \n",
       "42  43                   \"It's also a synonym for shit. \"   \n",
       "43  44  \"Well then you're welcome to contact your own ...   \n",
       "44  45  \"\\xa0@Presidentjlh\\xa0\\xa0@JenBroflovski you c...   \n",
       "45  46  \"Jimmy, your pop ups and fly outs are the reas...   \n",
       "46  47  \"Well i got on here, but i think im might just...   \n",
       "47  48                                \"You are so strong\"   \n",
       "48  49  \"Coach, please stop trying to explain this awa...   \n",
       "49  50  \"Learn reading comprehension. Sure Urlacher is...   \n",
       "\n",
       "                                     ProcessedComment  CommentLength  \n",
       "0                                      like tribe fan             34  \n",
       "1                                               idiot             37  \n",
       "2   woman babs war woman see coming jackazzes like...            201  \n",
       "3        wow benefitted many win year bat nice stupid             70  \n",
       "4              haha green red loser who winning moron             56  \n",
       "5           god difference refrigerator fart put meat            144  \n",
       "6   oh go kiss as goat dummycraps insult veteran e...            185  \n",
       "7                                    chance kid wrong             33  \n",
       "8                          real shit fuck live jasmin             39  \n",
       "9   ok hell released copy article pther anyone fuc...            165  \n",
       "10              crankyvince found slice internet fuck             70  \n",
       "11                                   delusional moron             44  \n",
       "12                             missing feeling better             70  \n",
       "13  made prediction like know anything football ma...            231  \n",
       "14  kathy lee shut pie hole whats face fish lip du...             87  \n",
       "15                                              crank             17  \n",
       "16                                   come oppa man xd             31  \n",
       "17          like enthusiasm initiative real go getter             65  \n",
       "18                              juggalo suck trolling             31  \n",
       "19                    seem eloquent grounded educated             61  \n",
       "20      wild wild world keep dirty finger human right             72  \n",
       "21                                     recapture swag             39  \n",
       "22                                          shit must             39  \n",
       "23  tenho pra esse tipo de babaca que acha que se ...            257  \n",
       "24                                             chance             30  \n",
       "25                                   disgusting human             38  \n",
       "26               well versed may one favs know detail             96  \n",
       "27                                       mouse pocket             40  \n",
       "28                                       glad realize             39  \n",
       "29                               damn would rock hard             36  \n",
       "30                                    face cano hater             27  \n",
       "31                                       foul kidding             35  \n",
       "32                       would know alot roll fat boy             42  \n",
       "33                              look pretty damn good             32  \n",
       "34  moron computer guy company mouth get trouble d...            217  \n",
       "35  corrupt government blasting underground bunker...            257  \n",
       "36                     phillies terrible go back cave             49  \n",
       "37                                 naive useful idiot             36  \n",
       "38                                            kidding             21  \n",
       "39  opinionated uneducated idiot kingdom heaven di...             98  \n",
       "40                             shut muffin soft b tch             35  \n",
       "41  okay going blow fucking mind three elderly mem...            420  \n",
       "42                                       also synonym             32  \n",
       "43                        well welcome contact lawyer             54  \n",
       "44                              jenbroflovski ask mom             71  \n",
       "45           jimmy pop ups fly out reason never happy             71  \n",
       "46  well got think im might get back see delusion ...             93  \n",
       "47                                             strong             19  \n",
       "48  coach please stop trying explain away deeper h...            105  \n",
       "49  learn reading comprehension sure urlacher aver...            396  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ProcessedComment'] = df['TextComment'].apply(nlp_preprocessing)\n",
    "\n",
    "#Only the relvant columns\n",
    "\n",
    "df = df[['id', 'TextComment', 'ProcessedComment','CommentLength']]\n",
    "df.head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransposedFont' object has no attribute 'getbbox'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-f9d7af4b66dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"white\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ProcessedComment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordcloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bilinear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'off'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \"\"\"\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    619\u001b[0m         \"\"\"\n\u001b[0;32m    620\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    622\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    451\u001b[0m                 \u001b[0mfont_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m                 self.generate_from_frequencies(dict(frequencies[:2]),\n\u001b[0m\u001b[0;32m    454\u001b[0m                                                max_font_size=self.height)\n\u001b[0;32m    455\u001b[0m                 \u001b[1;31m# find font sizes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    506\u001b[0m                     font, orientation=orientation)\n\u001b[0;32m    507\u001b[0m                 \u001b[1;31m# get size of resulting text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m                 \u001b[0mbox_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextbbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransposed_font\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"lt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m                 \u001b[1;31m# find possible places using integral image:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m                 result = occupancy.sample_position(box_size[3] + self.margin,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\ImageDraw.py\u001b[0m in \u001b[0;36mtextbbox\u001b[1;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[0;32m    565\u001b[0m             \u001b[0mfont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetfont\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"RGBA\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0membedded_color\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfontmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m         bbox = font.getbbox(\n\u001b[0m\u001b[0;32m    568\u001b[0m             \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstroke_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TransposedFont' object has no attribute 'getbbox'"
     ]
    }
   ],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", max_words=200, width=400, height=400, random_state=1).generate(' '.join(df['ProcessedComment']))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X = tfidf.fit_transform(df['ProcessedComment'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
